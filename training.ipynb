{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f98bba5-2267-4f3a-a03a-1cf3c57e2d60",
   "metadata": {},
   "source": [
    "# Rainbow Trading Agent\n",
    "\n",
    "Pytorch implementation based on original Tensorflow implementation from Clement Perroud\n",
    "\n",
    "1. https://github.com/ClementPerroud/RL-Trading-Agent\n",
    "2. https://github.com/ClementPerroud/Rainbow-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c519cf6-c060-4eb7-813a-bfe8cb62a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "from rainbow.agent import Rainbow\n",
    "\n",
    "import sys\n",
    "import gym_trading_env\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c5f65-1ce0-4da6-85e5-fec0b9db2d2a",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a894a9cb-65af-4b82-b81f-59f5eef66a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  diectory: data/processed/training/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/training/*.pkl\n",
      "Using  diectory: data/processed/training/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/training/*.pkl\n",
      "Using  diectory: data/processed/training/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/training/*.pkl\n",
      "Using  diectory: data/processed/training/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/training/*.pkl\n",
      "Using  diectory: data/processed/training/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/training/*.pkl\n",
      "Using  diectory: data/processed/validation/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/validation/*.pkl\n",
      "Using  diectory: data/processed/validation/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/validation/*.pkl\n",
      "Using  diectory: data/processed/validation/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/validation/*.pkl\n",
      "Using  diectory: data/processed/validation/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/validation/*.pkl\n",
      "Using  diectory: data/processed/validation/*.pkl\n",
      "Using dataset directory: /Users/arcmode/code/trading-rl/data/processed/validation/*.pkl\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    df[\"feature_close\"] = robust_scale(df[\"close\"].pct_change())\n",
    "    df[\"feature_open\"] = robust_scale(df[\"open\"]/df[\"close\"])\n",
    "    df[\"feature_high\"] = robust_scale(df[\"high\"]/df[\"close\"])\n",
    "    df[\"feature_low\"] = robust_scale(df[\"low\"]/df[\"close\"])\n",
    "    df[\"feature_volume\"] = robust_scale(df[\"volume\"] / df[\"volume\"].rolling(7*24).max())\n",
    "    df.dropna(inplace= True) # Clean your data!\n",
    "    return df\n",
    "\n",
    "\n",
    "def reward_function(history):\n",
    "    return 800*np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2]) #log (p_t / p_t-1 )\n",
    "\n",
    "def max_drawdown(history):\n",
    "    networth_array = history['portfolio_valuation']\n",
    "    _max_networth = networth_array[0]\n",
    "    _max_drawdown = 0\n",
    "    for networth in networth_array:\n",
    "        if networth > _max_networth:\n",
    "            _max_networth = networth\n",
    "        drawdown = ( networth - _max_networth ) / _max_networth\n",
    "        if drawdown < _max_drawdown:\n",
    "            _max_drawdown = drawdown\n",
    "    return f\"{_max_drawdown*100:5.2f}%\"\n",
    "\n",
    "def make_env(dir):\n",
    "    print(f\"Using  diectory: {dir}\")\n",
    "    dataset_dir = os.path.join(os.getcwd(), dir)\n",
    "    print(f\"Using dataset directory: {dataset_dir}\")\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir= dir,\n",
    "        preprocess= add_features,\n",
    "        windows= 15,\n",
    "        positions = [ -1, -0.5, 0, 1, 2], # From -1 (=SHORT), to +1 (=LONG)\n",
    "        initial_position = 0,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (= 1h here)\n",
    "        reward_function = reward_function,\n",
    "        portfolio_initial_value = 1000, # here, in USDT\n",
    "        \n",
    "        verbose= 1,\n",
    "    )\n",
    "    env.unwrapped.add_metric('Position Changes', lambda history : f\"{ 100*np.sum(np.diff(history['position']) != 0)/len(history['position']):5.2f}%\" )\n",
    "    env.unwrapped.add_metric('Max Drawdown', max_drawdown)\n",
    "    return env\n",
    "\n",
    "\n",
    "training_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"data/processed/training/*.pkl\") for _ in range(5)])\n",
    "validation_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"data/processed/validation/*.pkl\") for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7e5c7-b44e-4f96-b1fd-2eeb96daba8b",
   "metadata": {},
   "source": [
    "## Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0117cfce-f870-4480-818b-6091b666ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Rainbow(\n",
    "    simultaneous_training_env = 5,\n",
    "    \n",
    "    #Distributional\n",
    "    distributional= True,\n",
    "    v_min= -200,\n",
    "    v_max = 250,\n",
    "    nb_atoms= 51, \n",
    "    # Prioritized Replay\n",
    "    prioritized_replay = False,\n",
    "    prioritized_replay_alpha= 0.5,\n",
    "    prioritized_replay_beta_function = lambda episode, step : min(1, 0.5 + 0.5*step/150_000),\n",
    "    \n",
    "    # General\n",
    "    multi_steps = 3,\n",
    "    nb_states = 7,\n",
    "    nb_actions = 4,\n",
    "    gamma = 0.99,\n",
    "    replay_capacity = 1E8,\n",
    "    tau = 2000,\n",
    "    \n",
    "    # Model\n",
    "    window= 15,\n",
    "    units = [16,16, 16],\n",
    "    dropout= 0.2,\n",
    "    adversarial= True,\n",
    "    noisy= False,\n",
    "    learning_rate = 3*2.5E-4,\n",
    "\n",
    "    batch_size= 128,\n",
    "    train_every = 10,\n",
    "    epsilon_function = lambda episode, step : max(0.001, (1 - 5E-5)** step),\n",
    "    name = \"Rainbow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c2552-cf5c-4386-a29c-deec9c46dffc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d4272dc-7b8f-4512-9db7-19433ad2bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(steps = 100_000):\n",
    "    print(\"___________________________________________ TRAINING ___________________________________________\")\n",
    "    if 'obs' not in globals():\n",
    "        global obs\n",
    "        obs, info = training_envs.reset()\n",
    "    for _ in range(steps):\n",
    "        actions = agent.e_greedy_pick_actions(obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = training_envs.step(actions)\n",
    "\n",
    "        agent.store_replays(obs, actions, rewards, next_obs, dones, truncateds)\n",
    "        agent.train()\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "def evaluation():\n",
    "    print(\"___________________________________________ VALIDATION ___________________________________________\")\n",
    "    val_obs, info = validation_envs.reset()\n",
    "    check = np.array([False for _ in range(val_obs.shape[0])])\n",
    "    while not np.all(check):\n",
    "        actions = agent.e_greedy_pick_actions(val_obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = validation_envs.step(actions)\n",
    "        val_obs = next_obs\n",
    "        check += dones + truncateds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7571b-aad6-46d2-a614-bc9559446648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : -10.01%   |   Portfolio Return :  8.56%   |   Position Changes : 38.27%   |   Max Drawdown : -36.28%   |   \n",
      "Market Return : 40.05%   |   Portfolio Return : -20.20%   |   Position Changes : 39.31%   |   Max Drawdown : -31.43%   |   \n",
      "Market Return : 40.81%   |   Portfolio Return : -9.98%   |   Position Changes : 39.89%   |   Max Drawdown : -33.82%   |   \n",
      "Market Return : 39.79%   |   Portfolio Return : -0.63%   |   Position Changes : 38.80%   |   Max Drawdown : -21.49%   |   \n",
      "Market Return : -7.78%   |   Portfolio Return : -45.42%   |   Position Changes : 39.29%   |   Max Drawdown : -56.35%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : -10.04%   |   Portfolio Return : -28.78%   |   Position Changes : 21.35%   |   Max Drawdown : -34.64%   |   \n",
      "Market Return : -10.20%   |   Portfolio Return : -39.41%   |   Position Changes : 21.45%   |   Max Drawdown : -43.51%   |   \n",
      "Market Return : -10.20%   |   Portfolio Return : -34.86%   |   Position Changes : 21.53%   |   Max Drawdown : -39.96%   |   \n",
      "Market Return : -7.61%   |   Portfolio Return : -14.28%   |   Position Changes : 21.92%   |   Max Drawdown : -23.70%   |   \n",
      "Market Return : 43.46%   |   Portfolio Return : -8.57%   |   Position Changes : 21.03%   |   Max Drawdown : -23.35%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 2654.24%   |   Portfolio Return : -99.09%   |   Position Changes : 38.68%   |   Max Drawdown : -99.32%   |   \n",
      "↳ Env 1 : 000 :    88936   |   00:05:38   |   Epsilon : 1.17%   |   Mean Loss (last 10k) : 1.0983E+00   |   Tot. Rewards : -3757.73   |   Rewards (/1000 steps) :   -42.25   |   Length :  88936\n",
      "Market Return : 2669.06%   |   Portfolio Return : -98.61%   |   Position Changes : 38.52%   |   Max Drawdown : -98.90%   |   \n",
      "↳ Env 0 : 000 :    88953   |   00:05:39   |   Epsilon : 1.17%   |   Mean Loss (last 10k) : 1.0982E+00   |   Tot. Rewards : -3418.42   |   Rewards (/1000 steps) :   -38.43   |   Length :  88953\n",
      "Market Return : 2669.52%   |   Portfolio Return : -99.29%   |   Position Changes : 38.54%   |   Max Drawdown : -99.40%   |   \n",
      "↳ Env 2 : 000 :    88995   |   00:05:39   |   Epsilon : 1.17%   |   Mean Loss (last 10k) : 1.0982E+00   |   Tot. Rewards : -3956.47   |   Rewards (/1000 steps) :   -44.46   |   Length :  88995\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : -10.04%   |   Portfolio Return : -34.05%   |   Position Changes : 37.58%   |   Max Drawdown : -54.34%   |   \n",
      "Market Return : -10.09%   |   Portfolio Return : -41.75%   |   Position Changes : 38.28%   |   Max Drawdown : -58.52%   |   \n",
      "Market Return : -10.09%   |   Portfolio Return : -40.95%   |   Position Changes : 36.91%   |   Max Drawdown : -52.79%   |   \n",
      "Market Return : -7.66%   |   Portfolio Return : -46.69%   |   Position Changes : 40.20%   |   Max Drawdown : -56.19%   |   \n",
      "Market Return : -7.79%   |   Portfolio Return : -23.68%   |   Position Changes : 39.45%   |   Max Drawdown : -44.48%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 1773.47%   |   Portfolio Return : -99.56%   |   Position Changes : 38.19%   |   Max Drawdown : -99.58%   |   \n",
      "↳ Env 3 : 000 :    94242   |   00:06:15   |   Epsilon : 0.90%   |   Mean Loss (last 10k) : 1.0948E+00   |   Tot. Rewards : -4340.03   |   Rewards (/1000 steps) :   -46.05   |   Length :  94242\n",
      "Market Return : 1768.29%   |   Portfolio Return : -99.50%   |   Position Changes : 38.11%   |   Max Drawdown : -99.58%   |   \n",
      "↳ Env 4 : 000 :    94243   |   00:06:15   |   Epsilon : 0.90%   |   Mean Loss (last 10k) : 1.0948E+00   |   Tot. Rewards : -4241.69   |   Rewards (/1000 steps) :   -45.01   |   Length :  94243\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : -10.06%   |   Portfolio Return : -43.60%   |   Position Changes : 24.19%   |   Max Drawdown : -53.51%   |   \n",
      "Market Return : 39.14%   |   Portfolio Return : -6.79%   |   Position Changes : 24.37%   |   Max Drawdown : -31.39%   |   \n",
      "Market Return : 39.79%   |   Portfolio Return : -0.15%   |   Position Changes : 23.84%   |   Max Drawdown : -37.32%   |   \n",
      "Market Return : 43.53%   |   Portfolio Return : -18.22%   |   Position Changes : 23.10%   |   Max Drawdown : -36.46%   |   \n",
      "Market Return : 43.46%   |   Portfolio Return : -48.33%   |   Position Changes : 22.69%   |   Max Drawdown : -50.95%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : -10.06%   |   Portfolio Return : -5.24%   |   Position Changes : 24.62%   |   Max Drawdown : -30.03%   |   \n",
      "Market Return : 39.14%   |   Portfolio Return : -11.33%   |   Position Changes : 25.01%   |   Max Drawdown : -22.22%   |   \n",
      "Market Return : 39.79%   |   Portfolio Return : -5.23%   |   Position Changes : 25.37%   |   Max Drawdown : -25.33%   |   \n",
      "Market Return : -7.66%   |   Portfolio Return : -25.18%   |   Position Changes : 27.15%   |   Max Drawdown : -30.72%   |   \n",
      "Market Return : 43.46%   |   Portfolio Return :  4.11%   |   Position Changes : 27.08%   |   Max Drawdown : -23.21%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 1575.14%   |   Portfolio Return : 92.33%   |   Position Changes : 31.52%   |   Max Drawdown : -73.41%   |   \n",
      "↳ Env 1 : 001 :   178670   |   00:12:45   |   Epsilon : 0.10%   |   Mean Loss (last 10k) : 1.0422E+00   |   Tot. Rewards :   523.22   |   Rewards (/1000 steps) :     5.83   |   Length :  89734\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : 39.80%   |   Portfolio Return : -9.07%   |   Position Changes : 40.00%   |   Max Drawdown : -32.41%   |   \n",
      "Market Return : -7.78%   |   Portfolio Return : -27.74%   |   Position Changes : 39.66%   |   Max Drawdown : -59.61%   |   \n",
      "Market Return : -7.79%   |   Portfolio Return : -24.54%   |   Position Changes : 39.58%   |   Max Drawdown : -42.69%   |   \n",
      "Market Return : 43.61%   |   Portfolio Return : -33.43%   |   Position Changes : 38.12%   |   Max Drawdown : -44.67%   |   \n",
      "Market Return : -7.69%   |   Portfolio Return : -28.72%   |   Position Changes : 40.51%   |   Max Drawdown : -53.63%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 2306.88%   |   Portfolio Return : -86.55%   |   Position Changes : 32.35%   |   Max Drawdown : -93.31%   |   \n",
      "↳ Env 0 : 001 :   183196   |   00:13:22   |   Epsilon : 0.10%   |   Mean Loss (last 10k) : 1.0403E+00   |   Tot. Rewards : -1604.70   |   Rewards (/1000 steps) :   -17.03   |   Length :  94243\n",
      "Market Return : 1773.47%   |   Portfolio Return : -51.77%   |   Position Changes : 32.08%   |   Max Drawdown : -89.68%   |   \n",
      "Market Return : 2669.52%   |   Portfolio Return : -60.68%   |   Position Changes : 31.84%   |   Max Drawdown : -94.72%   |   \n",
      "↳ Env 2 : 001 :   183239   |   00:13:23   |   Epsilon : 0.10%   |   Mean Loss (last 10k) : 1.0403E+00   |   Tot. Rewards :  -583.35   |   Rewards (/1000 steps) :    -6.19   |   Length :  94244\n",
      "↳ Env 3 : 001 :   183239   |   00:13:23   |   Epsilon : 0.10%   |   Mean Loss (last 10k) : 1.0403E+00   |   Tot. Rewards :  -746.75   |   Rewards (/1000 steps) :    -8.39   |   Length :  88997\n",
      "Market Return : 1773.47%   |   Portfolio Return : 11.01%   |   Position Changes : 33.67%   |   Max Drawdown : -75.69%   |   \n",
      "↳ Env 4 : 001 :   188487   |   00:13:48   |   Epsilon : 0.10%   |   Mean Loss (last 10k) : 1.0389E+00   |   Tot. Rewards :    83.54   |   Rewards (/1000 steps) :     0.89   |   Length :  94244\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : 40.81%   |   Portfolio Return : -16.87%   |   Position Changes : 28.62%   |   Max Drawdown : -26.62%   |   \n",
      "Market Return : 39.79%   |   Portfolio Return : -6.48%   |   Position Changes : 27.07%   |   Max Drawdown : -22.19%   |   \n",
      "Market Return : 43.48%   |   Portfolio Return :  4.71%   |   Position Changes : 28.99%   |   Max Drawdown : -23.26%   |   \n",
      "Market Return : -7.71%   |   Portfolio Return : -19.65%   |   Position Changes : 30.86%   |   Max Drawdown : -36.64%   |   \n",
      "Market Return : 43.53%   |   Portfolio Return : -5.56%   |   Position Changes : 29.89%   |   Max Drawdown : -20.35%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : 39.80%   |   Portfolio Return :  0.93%   |   Position Changes : 34.25%   |   Max Drawdown : -28.90%   |   \n",
      "Market Return : 43.61%   |   Portfolio Return : -20.88%   |   Position Changes : 33.60%   |   Max Drawdown : -39.52%   |   \n",
      "Market Return : 43.46%   |   Portfolio Return : -31.40%   |   Position Changes : 34.01%   |   Max Drawdown : -50.08%   |   \n",
      "Market Return : 43.46%   |   Portfolio Return : -24.35%   |   Position Changes : 34.31%   |   Max Drawdown : -43.75%   |   \n",
      "Market Return : 43.48%   |   Portfolio Return : -19.22%   |   Position Changes : 34.29%   |   Max Drawdown : -41.32%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 30_000\n",
    "while True:\n",
    "    train(steps = NUM_STEPS)\n",
    "    evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0f1da-389e-472d-8099-2e5e14440cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill, pickle\n",
    "#agent.model = None\n",
    "#agent.target_model = None\n",
    "#agent.replay_memory = None\n",
    "\n",
    "with open(\"test.pkl\", \"wb\") as file:\n",
    "    dill.dump(agent, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34beb7-23b3-486e-84a0-f0fb7f3cb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(256):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be47996-d8e8-46d4-8b7c-b6e97196cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(1):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
